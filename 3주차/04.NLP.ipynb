{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid') # sns에 흰색 그리드 유지\n",
    "import missingno # 결측치 시각화\n",
    "\n",
    "# KFold (교차 검증을 사용하기 위해)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 교차검증 함수\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 학습 데이터와 검증 데이터로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 전처리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 평가 함수\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import  XGBRegressor\n",
    "\n",
    "# 머신러닝 알고리즘 - 군집\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# 머신러닝 알고리즘 - 차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 딥러닝 알고리즘 \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# 다중분류를 위한 원핫 인코더\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 학습 자동 중단\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 모델 저장\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 저장된 딥러닝 모델 불러오기\n",
    "from keras.models import load_model\n",
    "\n",
    "# 이미지 처리\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# 자연어 처리\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 시간 모듈\n",
    "import time\n",
    "\n",
    "# 그래프 설정\n",
    "# plt.rcParams['font.family'] = 'Malgun Gothic'   # 윈도우용\n",
    "plt.rcParams['font.family'] = 'AppleGothic'   # 맥용\n",
    "plt.rcParams['font.size'] = 10                 # 폰트 크기\n",
    "plt.rcParams['figure.figsize'] = 10,8          # 그래프 크기\n",
    "plt.rcParams['axes.unicode_minus'] = False     # - 기호 깨짐 방지\n",
    "\n",
    "\n",
    "# 경고 메시지가 안나오게 하기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문자열\n",
    "text = '해보지 않으면 해낼 수 없다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : 해보지 않으면 해낼 수 없다\n",
      "토큰화 : ['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "# 해당 텍스트를 토큰화 한다.\n",
    "result = text_to_word_sequence(text)\n",
    "print(f'원본 : {text}')\n",
    "print(f'토큰화 : {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    '먼저 텍스트의 각 단어를 나누어 토큰화 합니다',\n",
    "    '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다',\n",
    "    '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수를 통해 정리를 한다.\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('먼저', 1),\n",
       "             ('텍스트의', 2),\n",
       "             ('각', 1),\n",
       "             ('단어를', 1),\n",
       "             ('나누어', 1),\n",
       "             ('토큰화', 3),\n",
       "             ('합니다', 1),\n",
       "             ('단어로', 1),\n",
       "             ('해야', 1),\n",
       "             ('딥러닝에서', 2),\n",
       "             ('인식됩니다', 1),\n",
       "             ('한', 1),\n",
       "             ('결과는', 1),\n",
       "             ('사용할', 1),\n",
       "             ('수', 1),\n",
       "             ('있습니다', 1)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도수\n",
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문장의 개수\n",
    "token.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'먼저': 1,\n",
       "             '각': 1,\n",
       "             '나누어': 1,\n",
       "             '단어를': 1,\n",
       "             '토큰화': 3,\n",
       "             '합니다': 1,\n",
       "             '텍스트의': 2,\n",
       "             '해야': 1,\n",
       "             '딥러닝에서': 2,\n",
       "             '인식됩니다': 1,\n",
       "             '단어로': 1,\n",
       "             '있습니다': 1,\n",
       "             '사용할': 1,\n",
       "             '한': 1,\n",
       "             '결과는': 1,\n",
       "             '수': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어가 몇개의 문장에 포함되어 있는가.\n",
    "token.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'토큰화': 1,\n",
       " '텍스트의': 2,\n",
       " '딥러닝에서': 3,\n",
       " '먼저': 4,\n",
       " '각': 5,\n",
       " '단어를': 6,\n",
       " '나누어': 7,\n",
       " '합니다': 8,\n",
       " '단어로': 9,\n",
       " '해야': 10,\n",
       " '인식됩니다': 11,\n",
       " '한': 12,\n",
       " '결과는': 13,\n",
       " '사용할': 14,\n",
       " '수': 15,\n",
       " '있습니다': 16}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어의 부여된 인덱스\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가글 \n",
    "docs = [\n",
    "    '너무 재밌네요',\n",
    "    '최고예요',\n",
    "    '참 잘 만든 영화입니다',\n",
    "    '추천하고 싶은 영화입니다',\n",
    "    '한번 더 보고 싶네요',\n",
    "    '글쎄요',\n",
    "    '별로에요',\n",
    "    '생각보다 지루하네요',\n",
    "    '연기가 어색해요',\n",
    "    '재미없어요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 데이터\n",
    "# 긍정은 1, 부정은 0으로 선정\n",
    "classes = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs) # 토큰화 함수에 문장 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'영화입니다': 1,\n",
       " '너무': 2,\n",
       " '재밌네요': 3,\n",
       " '최고예요': 4,\n",
       " '참': 5,\n",
       " '잘': 6,\n",
       " '만든': 7,\n",
       " '추천하고': 8,\n",
       " '싶은': 9,\n",
       " '한번': 10,\n",
       " '더': 11,\n",
       " '보고': 12,\n",
       " '싶네요': 13,\n",
       " '글쎄요': 14,\n",
       " '별로에요': 15,\n",
       " '생각보다': 16,\n",
       " '지루하네요': 17,\n",
       " '연기가': 18,\n",
       " '어색해요': 19,\n",
       " '재미없어요': 20}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3],\n",
       " [4],\n",
       " [5, 6, 7, 1],\n",
       " [8, 9, 1],\n",
       " [10, 11, 12, 13],\n",
       " [14],\n",
       " [15],\n",
       " [16, 17],\n",
       " [18, 19],\n",
       " [20]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장을 토큰화 시킨 데이터를 단어 인덱스로 변환\n",
    "x = token.texts_to_sequences(docs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  4],\n",
       "       [ 5,  6,  7,  1],\n",
       "       [ 0,  8,  9,  1],\n",
       "       [10, 11, 12, 13],\n",
       "       [ 0,  0,  0, 14],\n",
       "       [ 0,  0,  0, 15],\n",
       "       [ 0,  0, 16, 17],\n",
       "       [ 0,  0, 18, 19],\n",
       "       [ 0,  0,  0, 20]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 리스트의 데이터의 개수를 최대 개수로 통일한다.\n",
    "padded_x = pad_sequences(x, 4)\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 수를 파악한다.\n",
    "word_size = len(token.word_index) + 1\n",
    "word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델을 구성한다.\n",
    "model = Sequential()\n",
    "# Embedding : 많은 단어의 수를 줄인다.\n",
    "# 첫번째 : 전체 단어의 수, 두번째 : 줄어든 사이즈\n",
    "# 세번째 : 입력층의 노드의 개수\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "# 1차원으로 변경\n",
    "model.add(Flatten())\n",
    "# 출력\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff19a5c7dc0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(padded_x, classes, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step - loss: 0.6255 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8999999761581421"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "model.evaluate(padded_x, classes)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
