{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수집할 페이지의 주소\n",
    "    - https://pjt3591oo.github.io/\n",
    "\n",
    "- user agent 확인\n",
    "   * http://m.avalon.co.kr/check.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 출력화면 초기화 하는 모듈\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요청하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSource(site):\n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36',   \n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers = header_info)\n",
    "#     print(response.text)\n",
    "\n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'https://pjt3591oo.github.io/'\n",
    "\n",
    "# getSource(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한 페이지의 데이터를 수집해 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(soup):\n",
    "    # 데이터가 있는 영역 전체를 가져온다.\n",
    "    a1 = soup.select_one('body > main > div > div')\n",
    "    \n",
    "    # 데이터가 있는 p태그를 가져온다\n",
    "    # a2 = a1.select('div.p') # 위에서 중복되는 부분 제거\n",
    "    a2 = a1.select('body > main > div > div > div)\n",
    "    \n",
    "    # 데이터를 담을 딕셔너리\n",
    "    data_dict ={\n",
    "        '제목':[],\n",
    "        '소제목':[],\n",
    "        '작성일자':[],\n",
    "        '작성자':[]\n",
    "    }\n",
    "    \n",
    "    # div 태그의 수만큼 반복한다.\n",
    "    for div in a2:\n",
    "        # 큰 제목 데이터\n",
    "        a3 = div.select_one('h3 > a')\n",
    "        data1 = a3.text.strip()\n",
    "        \n",
    "        # 소제목 데이터\n",
    "        a4 = div.select_one('h4')\n",
    "        data2 = a4.text.strip()\n",
    "        # 만약 빈 문자열이 추출됬다면 결측치로 변경\n",
    "        if len(data2) == 0:\n",
    "            data2 = np.nan\n",
    "        \n",
    "        \n",
    "        # 작성일자와 작성자 데이터\n",
    "        a5 = div.select_one('p > span')\n",
    "        data3 = a5.text.strip()\n",
    "        data4 = data3.split('|') \n",
    "        data5 = data4[0].strip() # 작성일자\n",
    "        data6 = data4[1].strip() # 작성자\n",
    "        \n",
    "        # 딕셔너리에 데이터를 담는다.\n",
    "        data_dict['제목'].append(data1)\n",
    "        data_dict['소제목'].append(data2)\n",
    "        data_dict['작성일자'].append(data5)\n",
    "        data_dict['작성자'].append(data6)\n",
    "        \n",
    "        \n",
    "    # 데이터 프레임 생성\n",
    "    df1 = pd.DataFrame(data_dict)\n",
    "     \n",
    "    # 데이터 프레임 저장\n",
    "    if os.path.exists('data1.csv') == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv('data1.csv', encoding='utf-8-sig', index=False) # 액셀에서 글자깨짐 방지\n",
    "    else:\n",
    "        df1.to_csv('data1.csv', encoding='utf-8-sig', index=False, header=False, mode='a') # 기존꺼 뒤에 추가해서 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음페이지 존재 여부 확인하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextPage(soup):\n",
    "    # next 버튼 태그를 가져온다.\n",
    "    a1 = soup.select_one('body > main > div > div > div.pagination > ul > li:nth-child(6) > a')\n",
    "\n",
    "    \n",
    "    # 만약 가져온 것이 있다면\n",
    "    if a1 != None:\n",
    "        # a 태그의 href 속성 값을 가져온다.\n",
    "        href = a1.attrs['href']\n",
    "        return href\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/page2/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site = 'https://pjt3591oo.github.io/'\n",
    "\n",
    "soup = getSource(site)\n",
    "getNextPage(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "site = 'https://pjt3591oo.github.io'\n",
    "page = ''\n",
    "\n",
    "while True:\n",
    "    # 딜레이\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 기존 출력된거 정리\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # 페이지 요청\n",
    "    soup = getSource(site+page)\n",
    "    \n",
    "    # 데이터 수집\n",
    "    getData(soup)\n",
    "    \n",
    "    # 다음페이지 존재확인\n",
    "    page = getNextPage(soup)\n",
    "    \n",
    "    # 다음페이지가 없다면 중단\n",
    "    if page == None:\n",
    "        print('수집완료')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
